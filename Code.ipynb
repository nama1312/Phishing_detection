{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R44lN1uZKagr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "from scipy import stats\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Dataset.csv')"
      ],
      "metadata": {
        "id": "sXhFRIaILsfb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNzxpp4vLzuT",
        "outputId": "daf142e3-7fbf-4dec-fed1-f30bfb264ce1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Type', 'url_length', 'number_of_dots_in_url',\n",
              "       'having_repeated_digits_in_url', 'number_of_digits_in_url',\n",
              "       'number_of_special_char_in_url', 'number_of_hyphens_in_url',\n",
              "       'number_of_underline_in_url', 'number_of_slash_in_url',\n",
              "       'number_of_questionmark_in_url', 'number_of_equal_in_url',\n",
              "       'number_of_at_in_url', 'number_of_dollar_in_url',\n",
              "       'number_of_exclamation_in_url', 'number_of_hashtag_in_url',\n",
              "       'number_of_percent_in_url', 'domain_length', 'number_of_dots_in_domain',\n",
              "       'number_of_hyphens_in_domain', 'having_special_characters_in_domain',\n",
              "       'number_of_special_characters_in_domain', 'having_digits_in_domain',\n",
              "       'number_of_digits_in_domain', 'having_repeated_digits_in_domain',\n",
              "       'number_of_subdomains', 'having_dot_in_subdomain',\n",
              "       'having_hyphen_in_subdomain', 'average_subdomain_length',\n",
              "       'average_number_of_dots_in_subdomain',\n",
              "       'average_number_of_hyphens_in_subdomain',\n",
              "       'having_special_characters_in_subdomain',\n",
              "       'number_of_special_characters_in_subdomain',\n",
              "       'having_digits_in_subdomain', 'number_of_digits_in_subdomain',\n",
              "       'having_repeated_digits_in_subdomain', 'having_path', 'path_length',\n",
              "       'having_query', 'having_fragment', 'having_anchor', 'entropy_of_url',\n",
              "       'entropy_of_domain'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "\n",
        "def extract_features(url):\n",
        "    # URL length\n",
        "    url_length = len(url)\n",
        "\n",
        "    # Number of dots in URL\n",
        "    number_of_dots_in_url = url.count('.')\n",
        "\n",
        "    # Having repeated digits in URL\n",
        "    having_repeated_digits_in_url = 1 if re.search(r'(\\d)\\1{2,}', url) else 0\n",
        "\n",
        "    # Number of digits in URL\n",
        "    number_of_digits_in_url = len(re.findall(r'\\d', url))\n",
        "\n",
        "    # Number of special characters in URL\n",
        "    number_of_special_char_in_url = len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', url))\n",
        "\n",
        "    # Number of hyphens in URL\n",
        "    number_of_hyphens_in_url = url.count('-')\n",
        "\n",
        "    # Number of underscores in URL\n",
        "    number_of_underline_in_url = url.count('_')\n",
        "\n",
        "    # Number of slashes in URL\n",
        "    number_of_slash_in_url = url.count('/')\n",
        "\n",
        "    # Number of question marks in URL\n",
        "    number_of_questionmark_in_url = url.count('?')\n",
        "\n",
        "    # Number of equal signs in URL\n",
        "    number_of_equal_in_url = url.count('=')\n",
        "\n",
        "    # Number of at signs in URL\n",
        "    number_of_at_in_url = url.count('@')\n",
        "\n",
        "    # Number of dollar signs in URL\n",
        "    number_of_dollar_in_url = url.count('$')\n",
        "\n",
        "    # Number of exclamation marks in URL\n",
        "    number_of_exclamation_in_url = url.count('!')\n",
        "\n",
        "    # Number of hashtags in URL\n",
        "    number_of_hashtag_in_url = url.count('#')\n",
        "\n",
        "    # Number of percent signs in URL\n",
        "    number_of_percent_in_url = url.count('%')\n",
        "\n",
        "    # Extract domain details\n",
        "    ext = tldextract.extract(url)\n",
        "    domain = ext.domain\n",
        "    subdomain = ext.subdomain\n",
        "\n",
        "    # Domain length\n",
        "    domain_length = len(domain)\n",
        "\n",
        "    # Number of dots in domain\n",
        "    number_of_dots_in_domain = domain.count('.')\n",
        "\n",
        "    # Number of hyphens in domain\n",
        "    number_of_hyphens_in_domain = domain.count('-')\n",
        "\n",
        "    # Having special characters in domain\n",
        "    having_special_characters_in_domain = 1 if re.search(r'[!@#$%^&*(),.?\":{}|<>]', domain) else 0\n",
        "\n",
        "    # Number of special characters in domain\n",
        "    number_of_special_characters_in_domain = len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', domain))\n",
        "\n",
        "    # Having digits in domain\n",
        "    having_digits_in_domain = 1 if re.search(r'\\d', domain) else 0\n",
        "\n",
        "    # Number of digits in domain\n",
        "    number_of_digits_in_domain = len(re.findall(r'\\d', domain))\n",
        "\n",
        "    # Having repeated digits in domain\n",
        "    having_repeated_digits_in_domain = 1 if re.search(r'(\\d)\\1{2,}', domain) else 0\n",
        "\n",
        "    # Number of subdomains\n",
        "    number_of_subdomains = len(subdomain.split('.'))\n",
        "\n",
        "    # Having dot in subdomain\n",
        "    having_dot_in_subdomain = 1 if '.' in subdomain else 0\n",
        "\n",
        "    # Having hyphen in subdomain\n",
        "    having_hyphen_in_subdomain = 1 if '-' in subdomain else 0\n",
        "\n",
        "    # Average subdomain length\n",
        "    average_subdomain_length = np.mean([len(part) for part in subdomain.split('.')])\n",
        "\n",
        "    # Average number of dots in subdomain\n",
        "    average_number_of_dots_in_subdomain = number_of_dots_in_url / number_of_subdomains if number_of_subdomains > 0 else 0\n",
        "\n",
        "    # Average number of hyphens in subdomain\n",
        "    average_number_of_hyphens_in_subdomain = number_of_hyphens_in_url / number_of_subdomains if number_of_subdomains > 0 else 0\n",
        "\n",
        "    # Having special characters in subdomain\n",
        "    having_special_characters_in_subdomain = 1 if re.search(r'[!@#$%^&*(),.?\":{}|<>]', subdomain) else 0\n",
        "\n",
        "    # Number of special characters in subdomain\n",
        "    number_of_special_characters_in_subdomain = len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', subdomain))\n",
        "\n",
        "    # Having digits in subdomain\n",
        "    having_digits_in_subdomain = 1 if re.search(r'\\d', subdomain) else 0\n",
        "\n",
        "    # Number of digits in subdomain\n",
        "    number_of_digits_in_subdomain = len(re.findall(r'\\d', subdomain))\n",
        "\n",
        "    # Having repeated digits in subdomain\n",
        "    having_repeated_digits_in_subdomain = 1 if re.search(r'(\\d)\\1{2,}', subdomain) else 0\n",
        "\n",
        "    # Check if URL has a path\n",
        "    path = url.split(domain)[-1] if domain in url else \"\"\n",
        "    having_path = 1 if path else 0\n",
        "    path_length = len(path)\n",
        "\n",
        "    # Check if URL has a query\n",
        "    having_query = 1 if '?' in url else 0\n",
        "\n",
        "    # Check if URL has a fragment\n",
        "    having_fragment = 1 if '#' in url else 0\n",
        "\n",
        "    # Check if URL has an anchor\n",
        "    having_anchor = 1 if '#' in url else 0\n",
        "\n",
        "    # Entropy calculations\n",
        "    def calculate_entropy(string):\n",
        "        probabilities = [float(string.count(c)) / len(string) for c in dict.fromkeys(list(string))]\n",
        "        entropy = - sum([p * np.log2(p) for p in probabilities])\n",
        "        return entropy\n",
        "\n",
        "    entropy_of_url = calculate_entropy(url)\n",
        "    entropy_of_domain = calculate_entropy(domain)\n",
        "\n",
        "    # Return features as a list\n",
        "    return [\n",
        "        url_length, number_of_dots_in_url, having_repeated_digits_in_url,\n",
        "        number_of_digits_in_url, number_of_special_char_in_url, number_of_hyphens_in_url,\n",
        "        number_of_underline_in_url, number_of_slash_in_url, number_of_questionmark_in_url,\n",
        "        number_of_equal_in_url, number_of_at_in_url, number_of_dollar_in_url,\n",
        "        number_of_exclamation_in_url, number_of_hashtag_in_url, number_of_percent_in_url,\n",
        "        domain_length, number_of_dots_in_domain, number_of_hyphens_in_domain,\n",
        "        having_special_characters_in_domain, number_of_special_characters_in_domain,\n",
        "        having_digits_in_domain, number_of_digits_in_domain, having_repeated_digits_in_domain,\n",
        "        number_of_subdomains, having_dot_in_subdomain, having_hyphen_in_subdomain,\n",
        "        average_subdomain_length, average_number_of_dots_in_subdomain,\n",
        "        average_number_of_hyphens_in_subdomain, having_special_characters_in_subdomain,\n",
        "        number_of_special_characters_in_subdomain, having_digits_in_subdomain,\n",
        "        number_of_digits_in_subdomain, having_repeated_digits_in_subdomain, having_path,\n",
        "        path_length, having_query, having_fragment, having_anchor,\n",
        "        entropy_of_url, entropy_of_domain\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "0KJj4n_SL1u8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('Dataset.csv')\n",
        "\n",
        "\n",
        "# Data preprocessing\n",
        "X = data.drop('Type', axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Handle missing values if any\n",
        "X.fillna(0, inplace=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model preparation\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6LoIfoHMBfe",
        "outputId": "7086b5c0-c1bf-48f4-97b4-28bff0959bf9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.97      0.96     38569\n",
            "           1       0.97      0.95      0.96     35816\n",
            "\n",
            "    accuracy                           0.96     74385\n",
            "   macro avg       0.96      0.96      0.96     74385\n",
            "weighted avg       0.96      0.96      0.96     74385\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('Dataset.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "X = data.drop('Type', axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# If the target variable 'Type' is categorical, convert it to numeric values\n",
        "# using LabelEncoder if not done already\n",
        "if y.dtype == 'object' or y.dtype == 'category':\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Handle missing values if any\n",
        "X.fillna(0, inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model preparation using Logistic Regression\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optionally, print accuracy as well\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLy0nqeOMUzE",
        "outputId": "12495972-fa44-4623-d80b-f40fac4242a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.87      0.82     38569\n",
            "           1       0.84      0.73      0.78     35816\n",
            "\n",
            "    accuracy                           0.80     74385\n",
            "   macro avg       0.81      0.80      0.80     74385\n",
            "weighted avg       0.81      0.80      0.80     74385\n",
            "\n",
            "Accuracy: 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('Dataset.csv')\n",
        "\n",
        "# Data preprocessing\n",
        "X = data.drop('Type', axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# If the target variable 'Type' is categorical, convert it to numeric values\n",
        "# using LabelEncoder if not done already\n",
        "if y.dtype == 'object' or y.dtype == 'category':\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Handle missing values if any\n",
        "X.fillna(0, inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling is optional for Decision Trees but I am keeping it consistent\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model preparation using Decision Tree Classifier\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optionally, print accuracy as well\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQjYtdr8Mnxg",
        "outputId": "d24a77ef-0a3f-4745-c1a5-14013a28077e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95     38569\n",
            "           1       0.95      0.94      0.94     35816\n",
            "\n",
            "    accuracy                           0.95     74385\n",
            "   macro avg       0.95      0.95      0.95     74385\n",
            "weighted avg       0.95      0.95      0.95     74385\n",
            "\n",
            "Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of tuning max depth\n",
        "modelll = DecisionTreeClassifier(max_depth=5, random_state=42)  # Limiting the depth to 5\n",
        "modelll.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "# Predictions\n",
        "y_pred = modelll.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optionally, print accuracy as well\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jthdwb-vMuOB",
        "outputId": "7257f731-f16f-4832-c276-3d3ad72cfb5f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.90      0.84     38569\n",
            "           1       0.87      0.73      0.79     35816\n",
            "\n",
            "    accuracy                           0.82     74385\n",
            "   macro avg       0.83      0.81      0.81     74385\n",
            "weighted avg       0.82      0.82      0.82     74385\n",
            "\n",
            "Accuracy: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "data = pd.read_csv('Dataset.csv')\n",
        "# Data preprocessing\n",
        "X = data.drop('Type', axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# If the target variable 'Type' is categorical, convert it to numeric values\n",
        "# using LabelEncoder if not done already\n",
        "if y.dtype == 'object' or y.dtype == 'category':\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Handle missing values if any\n",
        "X.fillna(0, inplace=True)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling is crucial for SVM\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model preparation using Support Vector Classifier (SVC)\n",
        "model = SVC(kernel='linear', random_state=42)  # Using linear kernel\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Optionally, print accuracy as well\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "B1bXUStxMzmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('Dataset.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Type', axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define base models with increased max_iter for Logistic Regression\n",
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
        "    ('lr', LogisticRegression(max_iter=2000, solver='lbfgs')),  # Increased max_iter\n",
        "    ('dt', DecisionTreeClassifier())\n",
        "]\n",
        "\n",
        "# Generate meta-features\n",
        "def generate_meta_features(base_models, X_train, y_train, X_val):\n",
        "    meta_features = np.zeros((X_val.shape[0], len(base_models)))\n",
        "\n",
        "    for i, (name, model) in enumerate(base_models):\n",
        "        model.fit(X_train, y_train)\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            meta_features[:, i] = model.predict_proba(X_val)[:, 1]  # Get probability of positive class\n",
        "        else:\n",
        "            meta_features[:, i] = model.predict(X_val)  # Use predict if predict_proba is not available\n",
        "\n",
        "    return meta_features\n",
        "\n",
        "# Split data for cross-validation and meta-feature generation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "meta_train = np.zeros((X_train_scaled.shape[0], len(base_models)))\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X_train_np = X_train_scaled\n",
        "y_train_np = y_train.values\n",
        "\n",
        "for train_index, val_index in kf.split(X_train_np):\n",
        "    X_tr, X_val = X_train_np[train_index], X_train_np[val_index]\n",
        "    y_tr, y_val = y_train_np[train_index], y_train_np[val_index]\n",
        "\n",
        "    meta_train[val_index] = generate_meta_features(base_models, X_tr, y_tr, X_val)\n",
        "\n",
        "# Train the meta-model\n",
        "meta_model = GradientBoostingClassifier(n_estimators=100)\n",
        "meta_model.fit(meta_train, y_train_np)\n",
        "\n",
        "# Now for test set, generate meta-features again\n",
        "meta_test = generate_meta_features(base_models, X_train_np, y_train_np, X_test_scaled)\n",
        "final_predictions = meta_model.predict(meta_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, final_predictions)\n",
        "print(f\"Hybrid Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siioU3u7SFZR",
        "outputId": "6b19992d-e536-4268-ff25-1ddac727275d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid Model Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('Dataset.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Type', axis=1)\n",
        "y = data['Type']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the data for models that are sensitive to feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the voting classifier with updated Logistic Regression max_iter and scaling\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('rf', RandomForestClassifier()),\n",
        "    ('lr', LogisticRegression(max_iter=2000, solver='lbfgs')),  # Increased max_iter for convergence\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True))  # SVC with probability=True for soft voting\n",
        "], voting='soft')\n",
        "\n",
        "# Fit the voting classifier on the scaled training data\n",
        "voting_clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the scaled test data\n",
        "voting_predictions = voting_clf.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "voting_accuracy = accuracy_score(y_test, voting_predictions)\n",
        "print(f\"Voting Classifier Accuracy: {voting_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "0yE5KahRWfTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9CuttmFrW9jc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}