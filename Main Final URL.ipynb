{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5380c4-fa88-41b4-aad3-271142fb4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d6469c-117f-4f89-9341-69898371a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91b8dfab-45a2-4736-9c81-a59042ddf895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Type', 'url_length', 'number_of_dots_in_url',\n",
       "       'having_repeated_digits_in_url', 'number_of_digits_in_url',\n",
       "       'number_of_special_char_in_url', 'number_of_hyphens_in_url',\n",
       "       'number_of_underline_in_url', 'number_of_slash_in_url',\n",
       "       'number_of_questionmark_in_url', 'number_of_equal_in_url',\n",
       "       'number_of_at_in_url', 'number_of_dollar_in_url',\n",
       "       'number_of_exclamation_in_url', 'number_of_hashtag_in_url',\n",
       "       'number_of_percent_in_url', 'domain_length', 'number_of_dots_in_domain',\n",
       "       'number_of_hyphens_in_domain', 'having_special_characters_in_domain',\n",
       "       'number_of_special_characters_in_domain', 'having_digits_in_domain',\n",
       "       'number_of_digits_in_domain', 'having_repeated_digits_in_domain',\n",
       "       'number_of_subdomains', 'having_dot_in_subdomain',\n",
       "       'having_hyphen_in_subdomain', 'average_subdomain_length',\n",
       "       'average_number_of_dots_in_subdomain',\n",
       "       'average_number_of_hyphens_in_subdomain',\n",
       "       'having_special_characters_in_subdomain',\n",
       "       'number_of_special_characters_in_subdomain',\n",
       "       'having_digits_in_subdomain', 'number_of_digits_in_subdomain',\n",
       "       'having_repeated_digits_in_subdomain', 'having_path', 'path_length',\n",
       "       'having_query', 'having_fragment', 'having_anchor', 'entropy_of_url',\n",
       "       'entropy_of_domain'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8d9c854-b4aa-4bb3-a4e3-8145cc4ca00e",
   "metadata": {},
   "source": [
    "#random forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff67faf-71a2-4395-8aa2-55776f249584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tldextract\n",
    "\n",
    "\n",
    "def extract_features(url):\n",
    "    # URL length\n",
    "    url_length = len(url)\n",
    "\n",
    "    # Number of dots in URL\n",
    "    number_of_dots_in_url = url.count('.')\n",
    "\n",
    "    # Having repeated digits in URL\n",
    "    having_repeated_digits_in_url = 1 if re.search(r'(\\d)\\1{2,}', url) else 0\n",
    "\n",
    "    # Number of digits in URL\n",
    "    number_of_digits_in_url = len(re.findall(r'\\d', url))\n",
    "\n",
    "    # Number of special characters in URL\n",
    "    number_of_special_char_in_url = len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', url))\n",
    "\n",
    "    # Number of hyphens in URL\n",
    "    number_of_hyphens_in_url = url.count('-')\n",
    "\n",
    "    # Number of underscores in URL\n",
    "    number_of_underline_in_url = url.count('_')\n",
    "\n",
    "    # Number of slashes in URL\n",
    "    number_of_slash_in_url = url.count('/')\n",
    "\n",
    "    # Number of question marks in URL\n",
    "    number_of_questionmark_in_url = url.count('?')\n",
    "\n",
    "    # Number of equal signs in URL\n",
    "    number_of_equal_in_url = url.count('=')\n",
    "\n",
    "    # Number of at signs in URL\n",
    "    number_of_at_in_url = url.count('@')\n",
    "\n",
    "    # Number of dollar signs in URL\n",
    "    number_of_dollar_in_url = url.count('$')\n",
    "\n",
    "    # Number of exclamation marks in URL\n",
    "    number_of_exclamation_in_url = url.count('!')\n",
    "\n",
    "    # Number of hashtags in URL\n",
    "    number_of_hashtag_in_url = url.count('#')\n",
    "\n",
    "    # Number of percent signs in URL\n",
    "    number_of_percent_in_url = url.count('%')\n",
    "\n",
    "    # Extract domain details\n",
    "    ext = tldextract.extract(url)\n",
    "    domain = ext.domain\n",
    "    subdomain = ext.subdomain\n",
    "\n",
    "    # Domain length\n",
    "    domain_length = len(domain)\n",
    "\n",
    "    # Number of dots in domain\n",
    "    number_of_dots_in_domain = domain.count('.')\n",
    "\n",
    "    # Number of hyphens in domain\n",
    "    number_of_hyphens_in_domain = domain.count('-')\n",
    "\n",
    "    # Having special characters in domain\n",
    "    having_special_characters_in_domain = 1 if re.search(r'[!@#$%^&*(),.?\":{}|<>]', domain) else 0\n",
    "\n",
    "    # Number of special characters in domain\n",
    "    number_of_special_characters_in_domain = len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', domain))\n",
    "\n",
    "    # Having digits in domain\n",
    "    having_digits_in_domain = 1 if re.search(r'\\d', domain) else 0\n",
    "\n",
    "    # Number of digits in domain\n",
    "    number_of_digits_in_domain = len(re.findall(r'\\d', domain))\n",
    "\n",
    "    # Having repeated digits in domain\n",
    "    having_repeated_digits_in_domain = 1 if re.search(r'(\\d)\\1{2,}', domain) else 0\n",
    "\n",
    "    # Number of subdomains\n",
    "    number_of_subdomains = len(subdomain.split('.'))\n",
    "\n",
    "    # Having dot in subdomain\n",
    "    having_dot_in_subdomain = 1 if '.' in subdomain else 0\n",
    "\n",
    "    # Having hyphen in subdomain\n",
    "    having_hyphen_in_subdomain = 1 if '-' in subdomain else 0\n",
    "\n",
    "    # Average subdomain length\n",
    "    average_subdomain_length = np.mean([len(part) for part in subdomain.split('.')])\n",
    "\n",
    "    # Average number of dots in subdomain\n",
    "    average_number_of_dots_in_subdomain = number_of_dots_in_url / number_of_subdomains if number_of_subdomains > 0 else 0\n",
    "\n",
    "    # Average number of hyphens in subdomain\n",
    "    average_number_of_hyphens_in_subdomain = number_of_hyphens_in_url / number_of_subdomains if number_of_subdomains > 0 else 0\n",
    "\n",
    "    # Having special characters in subdomain\n",
    "    having_special_characters_in_subdomain = 1 if re.search(r'[!@#$%^&*(),.?\":{}|<>]', subdomain) else 0\n",
    "\n",
    "    # Number of special characters in subdomain\n",
    "    number_of_special_characters_in_subdomain = len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', subdomain))\n",
    "\n",
    "    # Having digits in subdomain\n",
    "    having_digits_in_subdomain = 1 if re.search(r'\\d', subdomain) else 0\n",
    "\n",
    "    # Number of digits in subdomain\n",
    "    number_of_digits_in_subdomain = len(re.findall(r'\\d', subdomain))\n",
    "\n",
    "    # Having repeated digits in subdomain\n",
    "    having_repeated_digits_in_subdomain = 1 if re.search(r'(\\d)\\1{2,}', subdomain) else 0\n",
    "\n",
    "    # Check if URL has a path\n",
    "    path = url.split(domain)[-1] if domain in url else \"\"\n",
    "    having_path = 1 if path else 0\n",
    "    path_length = len(path)\n",
    "\n",
    "    # Check if URL has a query\n",
    "    having_query = 1 if '?' in url else 0\n",
    "\n",
    "    # Check if URL has a fragment\n",
    "    having_fragment = 1 if '#' in url else 0\n",
    "\n",
    "    # Check if URL has an anchor\n",
    "    having_anchor = 1 if '#' in url else 0\n",
    "\n",
    "    # Entropy calculations\n",
    "    def calculate_entropy(string):\n",
    "        probabilities = [float(string.count(c)) / len(string) for c in dict.fromkeys(list(string))]\n",
    "        entropy = - sum([p * np.log2(p) for p in probabilities])\n",
    "        return entropy\n",
    "\n",
    "    entropy_of_url = calculate_entropy(url)\n",
    "    entropy_of_domain = calculate_entropy(domain)\n",
    "\n",
    "    # Return features as a list\n",
    "    return [\n",
    "        url_length, number_of_dots_in_url, having_repeated_digits_in_url,\n",
    "        number_of_digits_in_url, number_of_special_char_in_url, number_of_hyphens_in_url,\n",
    "        number_of_underline_in_url, number_of_slash_in_url, number_of_questionmark_in_url,\n",
    "        number_of_equal_in_url, number_of_at_in_url, number_of_dollar_in_url,\n",
    "        number_of_exclamation_in_url, number_of_hashtag_in_url, number_of_percent_in_url,\n",
    "        domain_length, number_of_dots_in_domain, number_of_hyphens_in_domain,\n",
    "        having_special_characters_in_domain, number_of_special_characters_in_domain,\n",
    "        having_digits_in_domain, number_of_digits_in_domain, having_repeated_digits_in_domain,\n",
    "        number_of_subdomains, having_dot_in_subdomain, having_hyphen_in_subdomain,\n",
    "        average_subdomain_length, average_number_of_dots_in_subdomain,\n",
    "        average_number_of_hyphens_in_subdomain, having_special_characters_in_subdomain,\n",
    "        number_of_special_characters_in_subdomain, having_digits_in_subdomain,\n",
    "        number_of_digits_in_subdomain, having_repeated_digits_in_subdomain, having_path,\n",
    "        path_length, having_query, having_fragment, having_anchor,\n",
    "        entropy_of_url, entropy_of_domain\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233b0fce-c77d-4cd3-9416-2f24affdcb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96     38569\n",
      "           1       0.97      0.95      0.96     35816\n",
      "\n",
      "    accuracy                           0.96     74385\n",
      "   macro avg       0.96      0.96      0.96     74385\n",
      "weighted avg       0.96      0.96      0.96     74385\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Dataset.csv')\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "X = data.drop('Type', axis=1)\n",
    "y = data['Type']\n",
    "\n",
    "# Handle missing values if any\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model preparation\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab8cb8a-b22f-4a79-8cfc-b1d64d2cb35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remaining samples after outlier removal: 0\n",
    "#All samples were removed by the outlier detection. Consider relaxing the filtering criteria.\n",
    "\n",
    "#hence didnot provided outlier treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0bae0d6-068e-47e1-a1a5-1915a9eddb3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model and scaler\n",
    "joblib.dump(model, 'phishing_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aca41f42-12fb-4291-b759-61fc5670f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "#  LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f094c9-774d-4c2e-b6e3-64219fbead8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82     38569\n",
      "           1       0.84      0.73      0.78     35816\n",
      "\n",
      "    accuracy                           0.80     74385\n",
      "   macro avg       0.81      0.80      0.80     74385\n",
      "weighted avg       0.81      0.80      0.80     74385\n",
      "\n",
      "Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Dataset.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "X = data.drop('Type', axis=1)\n",
    "y = data['Type']\n",
    "\n",
    "# If the target variable 'Type' is categorical, convert it to numeric values\n",
    "# using LabelEncoder if not done already\n",
    "if y.dtype == 'object' or y.dtype == 'category':\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Handle missing values if any\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model preparation using Logistic Regression\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, print accuracy as well\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "908d4377-db10-4609-8c73-55cd49bd008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "\n",
    "##########\n",
    "\n",
    "#### DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3172a7c-bec2-4e8d-ba82-54ecf1d98883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95     38569\n",
      "           1       0.95      0.94      0.94     35816\n",
      "\n",
      "    accuracy                           0.95     74385\n",
      "   macro avg       0.95      0.95      0.95     74385\n",
      "weighted avg       0.95      0.95      0.95     74385\n",
      "\n",
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('Dataset.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "X = data.drop('Type', axis=1)\n",
    "y = data['Type']\n",
    "\n",
    "# If the target variable 'Type' is categorical, convert it to numeric values\n",
    "# using LabelEncoder if not done already\n",
    "if y.dtype == 'object' or y.dtype == 'category':\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Handle missing values if any\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Feature scaling is optional for Decision Trees but I am keeping it consistent\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model preparation using Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, print accuracy as well\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0511aff-f568-4c2a-9a71-edaf6e408267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.84     38569\n",
      "           1       0.87      0.73      0.79     35816\n",
      "\n",
      "    accuracy                           0.82     74385\n",
      "   macro avg       0.83      0.81      0.81     74385\n",
      "weighted avg       0.82      0.82      0.82     74385\n",
      "\n",
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Example of tuning max depth\n",
    "modelll = DecisionTreeClassifier(max_depth=5, random_state=42)  # Limiting the depth to 5\n",
    "modelll.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred = modelll.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, print accuracy as well\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "710cfd74-42df-4ba5-9596-e808f786ee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "\n",
    "#########SVM ################\n",
    "\n",
    "\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f09e33ab-b4c8-4487-9235-f0d4ad1c6456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "\n",
    "#################stacking#########\n",
    "\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9eb78a1-7c88-497b-9b31-3c939b7662c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mishr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mishr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mishr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mishr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mishr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\mishr\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model Accuracy: 0.9617261544666263\n",
      "Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "# Function to generate meta-features\n",
    "def generate_meta_features(base_models, X_train, y_train, X_val):\n",
    "    meta_features = np.zeros((X_val.shape[0], len(base_models)))\n",
    "    \n",
    "    for i, (name, model) in enumerate(base_models):\n",
    "        model.fit(X_train, y_train)\n",
    "        ##\n",
    "        joblib.dump(model, f\"{name}_model.joblib\")\n",
    "        meta_features[:, i] = model.predict_proba(X_val)[:, 1]  # Get probability of positive class\n",
    "    \n",
    "    return meta_features\n",
    "\n",
    "# Split data for cross-validation and meta-feature generation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "meta_train = np.zeros((X_train.shape[0], len(base_models)))\n",
    "\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Use .iloc for proper row-based indexing with Pandas DataFrame\n",
    "    X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Generate meta-features for the validation fold\n",
    "    meta_train[val_index] = generate_meta_features(base_models, X_tr, y_tr, X_val)\n",
    "\n",
    "# Train the meta-model\n",
    "meta_model = GradientBoostingClassifier(n_estimators=100)\n",
    "meta_model.fit(meta_train, y_train)\n",
    "\n",
    "# Generate meta-features for the test set\n",
    "meta_test = generate_meta_features(base_models, X_train, y_train, X_test)\n",
    "final_predictions = meta_model.predict(meta_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, final_predictions)\n",
    "print(f\"Hybrid Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Save the meta-model\n",
    "joblib.dump(meta_model, \"meta_model.joblib\")\n",
    "\n",
    "print(\"Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f66a9-b971-447e-ab92-92ddf1fc9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "\n",
    "#########################################################################soft voting\n",
    "\n",
    "\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267fa8b-8e0f-42d4-8697-d05ff7fdc334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier()),\n",
    "    ('lr', LogisticRegression()),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('svm', SVC(probability=True))\n",
    "], voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_predictions = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "voting_accuracy = accuracy_score(y_test, voting_predictions)\n",
    "print(f\"Voting Classifier Accuracy: {voting_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0028456-3b77-4304-8202-f6d47158fbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccdf3b7-0768-4a81-b8a8-fb8b8357c950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5ac210-2d9d-4e88-b4f2-822e1d21ae5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
